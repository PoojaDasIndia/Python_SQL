{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64d40e31-5101-4718-b4be-83cb66bdc22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_199116\\170011551.py:32: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(executable_path=path)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from time import sleep\n",
    "\n",
    "website=[]\n",
    "phone=[]\n",
    "phone1=[]\n",
    "phone2=[]\n",
    "phone3=[]\n",
    "phone4=[]\n",
    "phone5=[]\n",
    "phone6=[]\n",
    "name=[]\n",
    "email_id=[]\n",
    "address=[]\n",
    "facebook=[]\n",
    "doamin_list=[]\n",
    "city=[]\n",
    "state=[]\n",
    "country=[]\n",
    "\n",
    "path=\"C:chromedriver_win32/chromedriver.exe\"\n",
    "\n",
    "\n",
    "driver=webdriver.Chrome(executable_path=path)\n",
    "c=0\n",
    "\n",
    "\n",
    "# for domain in link[0:15]:\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "#     response = requests.get(LinkedIn_link[i])\n",
    "\n",
    "#     # Create a BeautifulSoup object from the response content\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     # Find all the phone numbers on the page\n",
    "#     try:\n",
    "#         if soup.find('a', href=lambda href: href and 'tel:' in href):\n",
    "#             phone_numbers = soup.find_all('a', href=lambda href: href and 'tel:' in href)\n",
    "#             # phone_number = phones['href'].split(':')[1]\n",
    "#             # print(phone_number)\n",
    "#             h=[]\n",
    "#             for phones in phone_numbers:\n",
    "#                 phone_number = phones['href'].split(':')[1]\n",
    "#                 print(phone_number)\n",
    "#                 h.append(phone_number)\n",
    "#             print(h)    \n",
    "#             phone.append(h)\n",
    "#         else:\n",
    "#             phone.append('')\n",
    "#     except:\n",
    "#         phone.append('')\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "try:\n",
    "    # driver.get('https://www.google.com/search?q=%22Powered+and+secured+by+Wix%22&sxsrf=APwXEdfdXSRMOKV1v1YukJsRZoecgYVNqw:1687787474461&source=lnt&tbs=qdr:d&sa=X&ved=2ahUKEwit9PumiuH_AhWXyjgGHWjHBGQQpwV6BAgBEAg&biw=1280&bih=609&dpr=1.5#ip=1')\n",
    "    driver.get('http://svnutech.com/')\n",
    "    sleep(3)\n",
    "\n",
    "         # Load the website URL in the browser\n",
    "        # driver.get(website_url)\n",
    "\n",
    "        # Extract phone numbers using regular expressions\n",
    "    try:\n",
    "        phone_numbers = driver.find_elements(By.XPATH,\"//a[starts-with(@href, 'tel:')]\")\n",
    "        if phone_numbers:\n",
    "            phone.append([phone_number.get_attribute('href').split(':')[1] for phone_number in phone_numbers])\n",
    "        else:\n",
    "            phone.append('Nan')\n",
    "\n",
    "    except:\n",
    "        phone.append('Nan')\n",
    "    sleep(0.5)   \n",
    "\n",
    "        # Extract the first phone number using regular expressions\n",
    "    try:\n",
    "        phone_number_element = driver.find_element(By.XPATH,\"//a[starts-with(@href, 'tel:')][1]\")\n",
    "        phone_number = re.search(r'\\+91\\s?\\d{10}', phone_number_element.get_attribute('href'))\n",
    "        if phone_number:\n",
    "            phone1.append(phone_number.group(0))\n",
    "        else:\n",
    "            phone1.append('Nan')\n",
    "    except:\n",
    "        phone1.append('Nan')\n",
    "    sleep(0.5)   \n",
    "\n",
    "    # Extract phone numbers using regular expressions from the webpage content\n",
    "    try:\n",
    "        webpage_content = driver.page_source\n",
    "        phone_numbers = re.findall(r'\\+91\\s?\\d{10}', webpage_content)\n",
    "        if phone_numbers:\n",
    "            phone2.append(phone_numbers)\n",
    "        else:\n",
    "            phone2.append('Nan')\n",
    "    except:\n",
    "        phone2.append('Nan')\n",
    "            \n",
    "    sleep(0.5)   \n",
    "\n",
    "\n",
    "        # Extract Facebook link using regular expressions\n",
    "    try:\n",
    "        facebook_link_element = driver.find_element(By.XPATH,\"//a[contains(@href, 'facebook.com')]\")\n",
    "        facebook_link = facebook_link_element.get_attribute('href')\n",
    "        if facebook_link:\n",
    "            facebook.append(facebook_link)\n",
    "        else:\n",
    "            facebook.append('Nan')\n",
    "    except:\n",
    "        facebook.append('Nan')\n",
    "            \n",
    "    sleep(0.5)   \n",
    "\n",
    "    # Extract website main name using Selenium\n",
    "    try:\n",
    "        website_main_name = driver.title\n",
    "        name.append(website_main_name)\n",
    "    except:\n",
    "        name.append('Nan')\n",
    "            \n",
    "    sleep(0.5)   \n",
    "            \n",
    "    # Extract phone numbers using regular expressions from the webpage content\n",
    "            \n",
    "    try:\n",
    "        phone_pattern = r'\\b\\d{10}\\b'\n",
    "        phone_numbers = re.search(phone_pattern, phone_number)\n",
    "\n",
    "        if phone_numbers:\n",
    "            phone5.append(phone_numbers.group(0))\n",
    "        else:\n",
    "            phone5.append('Nan')\n",
    "    except:\n",
    "        phone5.append('Nan')\n",
    "            \n",
    "    sleep(0.5)   \n",
    "\n",
    "    # Extract email address using regular expressions\n",
    "    try:\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        email_element = driver.find_element(By.XPATH,\"//a[contains(@href, 'mailto:')]\")\n",
    "        email = re.search(email_pattern, email_element.get_attribute('href'))\n",
    "        if email:\n",
    "            email_id.append(email.group(0))\n",
    "        else:\n",
    "            email.append('Nan')\n",
    "    except:\n",
    "        email_id.append('Nan')\n",
    "            \n",
    "    sleep(0.5)   \n",
    "\n",
    "    # Extract address using regular expressions\n",
    "    try:\n",
    "        address_pattern = r'\\b\\d+[\\w\\s]+(?:Road|Street|Avenue|Lane)\\b'\n",
    "        address_element = driver.find_element(By.XPATH,\"//address\")\n",
    "        address = re.search(address_pattern, address_element.text)\n",
    "        if address:\n",
    "            address.append(address.group(0))\n",
    "        else:\n",
    "            address.append('Nan')\n",
    "    except:\n",
    "        address.append('Nan')\n",
    "            \n",
    "    sleep(0.5)   \n",
    "            \n",
    "    try:    \n",
    "        phone_numbers = re.findall(r'\\d{3}-\\d{3}-\\d{4}', phone_number)\n",
    "\n",
    "        if phone_numbers:\n",
    "            phone3.append(phone_numbers)\n",
    "        else:\n",
    "            phone3.append('Nan')\n",
    "    except:\n",
    "            phone3.append('Nan')\n",
    "                \n",
    "    sleep(0.5)   \n",
    "                \n",
    "                \n",
    "    try:\n",
    "        phone_pattern = r'\\+?\\d{1,3}[-.\\s]?\\(?\\d{1,3}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}'\n",
    "\n",
    "        phone_numbers = re.findall(phone_pattern, phone_number)\n",
    "\n",
    "        if phone_numbers:\n",
    "            phone4.append(phone_numbers)\n",
    "        else:\n",
    "            phone4.append('Nan')\n",
    "    except:\n",
    "        phone4.append('Nan')\n",
    "                \n",
    "    sleep(0.5)   \n",
    "                \n",
    "                \n",
    "    try:\n",
    "        phone_number = phone_number.replace(\"/\", \" \").replace(\"-\", \" \")\n",
    "        phone_pattern = r'(\\+[\\d\\s()-]+|\\([\\d\\s()-]+\\))\\s*\\d[\\d\\s()-]*\\d'\n",
    "\n",
    "        phone_numbers = re.search(phone_pattern, phone_number)\n",
    "\n",
    "        if phone_numbers:\n",
    "            phone6.append(phone_numbers)\n",
    "        else:\n",
    "            phone6.append('Nan')\n",
    "    except:\n",
    "        phone6.append('Nan')\n",
    "    sleep(0.5)           \n",
    "                \n",
    "                \n",
    "                \n",
    "#         try:\n",
    "#             pattern = r'(\\b[A-Z][a-zA-Z\\s]+),\\s(\\b[A-Z][a-zA-Z\\s]+),\\s([A-Z]{2,})$'\n",
    "\n",
    "#             match = re.search(pattern, text)\n",
    "\n",
    "#             if match:\n",
    "#                 cities = match.group(1)\n",
    "#                 states = match.group(2)\n",
    "#                 countrys = match.group(3)\n",
    "#                 city.append(cities)\n",
    "#                 state.append(states)\n",
    "#                 country.append(countries)\n",
    "                \n",
    "#             else:\n",
    "#                 city.append('Nan')\n",
    "#                 state.append('Nan')\n",
    "#                 country.append('Nan')\n",
    "#         except:\n",
    "#                 city.append('Nan')\n",
    "#                 state.append('Nan')\n",
    "#                 country.append('Nan')\n",
    "                \n",
    "        # Append the website URL\n",
    "    # website.append(f'https://{domain}/')\n",
    "    # doamin_list.append(domain)\n",
    "    # sleep(1)\n",
    "        \n",
    "except:\n",
    "    print(f'site not working')\n",
    "    \n",
    "# print(f\"Scrape web {d,{c}\")\n",
    "    # c+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13129c7a-4a2e-4821-bc54-42faffa60eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "email=[]\n",
    "phone1=[]\n",
    "phone2=[]\n",
    "phone3=[]\n",
    "phone4=[]\n",
    "phone5=[]\n",
    "phone6=[]\n",
    "linkedin=[]\n",
    "facebook=[]\n",
    "name=[]\n",
    "# Fetch the HTML content of the web page\n",
    "url = 'https://www.comptechdigital.in/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Scrape email addresses\n",
    "email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "email_matches = re.findall(email_pattern, html_content)\n",
    "email.append(email_matches)\n",
    "\n",
    "# Scrape phone numbers\n",
    "phone_pattern = r\"\\+\\d\\s\\d{3}\\s\\d{3}\\s\\d{4}\"\n",
    "phone_matches = re.findall(phone_pattern, html_content)\n",
    "phone1.append(phone_matches)\n",
    "# # Print scraped phone numbers\n",
    "# for phone_number in phone_matches:\n",
    "#     phone1.append(phone_number)\n",
    "    \n",
    "    \n",
    "    \n",
    "phone_pattern = r\"\\d{3}-\\d{3}-\\d{4}\"\n",
    "phone_matches = re.findall(phone_pattern, html_content)\n",
    "phone2.append(phone_matches)\n",
    "\n",
    "cleaned_content = re.sub(r'\\s|-', '', html_content)\n",
    "\n",
    "# Scrape phone numbers starting with \"+91\"\n",
    "phone_pattern = r\"\\+91\\d{10}\"\n",
    "phone_matches = re.findall(phone_pattern, cleaned_content)\n",
    "phone3.append(phone_matches)\n",
    "\n",
    "\n",
    "modified_text = re.sub(r'[\\s-]', '', html_content)\n",
    "\n",
    "# Scrape phone numbers starting with \"+\"\n",
    "phone_pattern = r\"\\+\\d+\"\n",
    "phone_matches = re.findall(phone_pattern, modified_text)\n",
    "phone4.append(phone_matches)\n",
    "\n",
    "phone_pattern = r\"\\+91\\s\\d{3}\\s\\d{3}\\s\\d{4}\"\n",
    "phone_matches = re.findall(phone_pattern, html_content)\n",
    "phone5.append(phone_matches)\n",
    "\n",
    "\n",
    "facebook_pattern = r\"https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9_\\-/]+\"\n",
    "facebook_links = re.findall(facebook_pattern, html_content)\n",
    "facebook.append(facebook_links)\n",
    "\n",
    "\n",
    "phone_pattern = r\"\\+91-\\d{10}\"\n",
    "phone_matches = re.findall(phone_pattern, html_content)\n",
    "phone6.append(phone_matches)\n",
    "\n",
    "linkedin_links = soup.find_all('a', href=re.compile(r'linkedin\\.com/in/'))\n",
    "\n",
    "# Extract the href attribute value from the anchor tags\n",
    "linkedin_profiles = [link['href'] for link in linkedin_links]\n",
    "\n",
    "linkedin.append(linkedin_profiles)\n",
    "\n",
    "title_tag = soup.find('title')\n",
    "\n",
    "# Extract the title\n",
    "title = title_tag.text.strip()\n",
    "\n",
    "name.append( title)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01038a26-7633-4d0b-a838-4ed4660dfc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(email))\n",
    "print(len(phone1))\n",
    "print(len(phone2))\n",
    "print(len(phone3))\n",
    "print(len(phone4))\n",
    "print(len(phone5))\n",
    "print(len(phone6))\n",
    "print(len(linkedin))\n",
    "print(len(facebook))\n",
    "print(len(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c01b0096-577d-4ff9-9bb2-74403cf81369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3ee0255-dffd-435e-b08e-4e2cd32fa1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() \n",
    "\n",
    "\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'Name':name,\n",
    "  \n",
    "     'Phone1':phone1,\n",
    "    'Phone2':phone2,\n",
    "    'Phone3':phone3,\n",
    "    'Phone4':phone4,\n",
    "    'Phone5':phone5,\n",
    "    'Phone6':phone6,\n",
    "    'email':email,\n",
    "    # 'Website':website,\n",
    "    'linkedin':linkedin,\n",
    "    'Facebook':facebook,\n",
    "    # 'doamin_list':doamin_list\n",
    "    \n",
    "               })\n",
    "\n",
    "df.to_csv(\"domain_email_facebook_startup.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c04df1-7443-4601-8bff-af931d64661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b04c8b5-b010-4dfb-ad97-e98e2e01abab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Phone1</th>\n",
       "      <th>Phone2</th>\n",
       "      <th>Phone3</th>\n",
       "      <th>Phone4</th>\n",
       "      <th>Phone5</th>\n",
       "      <th>Phone6</th>\n",
       "      <th>email</th>\n",
       "      <th>linkedin</th>\n",
       "      <th>Facebook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trusted Mobie App and Web Development Company|...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[+917303518100, +919773958145, +917303518100, ...</td>\n",
       "      <td>[+917303518100, +919773958145, +917303518100, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[+91-7303518100]</td>\n",
       "      <td>[comptech.delhi@gmail.com, comptech.delhi@gmai...</td>\n",
       "      <td>[https://in.linkedin.com/in/comptech-delhi]</td>\n",
       "      <td>[https://www.facebook.com/comptech, https://ww...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Phone1 Phone2  \\\n",
       "0  Trusted Mobie App and Web Development Company|...     []     []   \n",
       "\n",
       "                                              Phone3  \\\n",
       "0  [+917303518100, +919773958145, +917303518100, ...   \n",
       "\n",
       "                                              Phone4 Phone5            Phone6  \\\n",
       "0  [+917303518100, +919773958145, +917303518100, ...     []  [+91-7303518100]   \n",
       "\n",
       "                                               email  \\\n",
       "0  [comptech.delhi@gmail.com, comptech.delhi@gmai...   \n",
       "\n",
       "                                      linkedin  \\\n",
       "0  [https://in.linkedin.com/in/comptech-delhi]   \n",
       "\n",
       "                                            Facebook  \n",
       "0  [https://www.facebook.com/comptech, https://ww...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d9aa2a-df42-497b-8fba-f48790ef3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_199116\\1358364599.py:14: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(executable_path=path)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "path=\"C:chromedriver_win32/chromedriver.exe\"\n",
    "driver=webdriver.Chrome(executable_path=path)\n",
    "driver.get('https://www.google.com/search?q=%22Powered+and+secured+by+Wix%22&sxsrf=APwXEdfdXSRMOKV1v1YukJsRZoecgYVNqw:1687787474461&source=lnt&tbs=qdr:d&sa=X&ved=2ahUKEwit9PumiuH_AhWXyjgGHWjHBGQQpwV6BAgBEAg&biw=1280&bih=609&dpr=1.5#ip=1')\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80304571-6085-4ce6-b4cd-690bada064b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page scrape 1\n",
      "page scrape 2\n",
      "page scrape 3\n",
      "page scrape 4\n",
      "page scrape 5\n",
      "page scrape 6\n",
      "page scrape 7\n",
      "page scrape 8\n",
      "page scrape 9\n",
      "page scrape 10\n",
      "page scrape 11\n",
      "page scrape 12\n",
      "page scrape 13\n",
      "page scrape 14\n",
      "page scrape 15\n",
      "page scrape 16\n",
      "page scrape 17\n",
      "page scrape 18\n",
      "page scrape 19\n",
      "page scrape 20\n",
      "page scrape 21\n",
      "page scrape 22\n",
      "page scrape 23\n",
      "page scrape 24\n",
      "Page end\n"
     ]
    }
   ],
   "source": [
    "link_contact=[]\n",
    "link=[]\n",
    "name=[]\n",
    "c=1\n",
    "try:\n",
    "    for n in range(30):\n",
    "        links=driver.find_elements(By.XPATH,'//cite[@class=\"apx8Vc qLRx3b tjvcx GvPZzd cHaqb\"]')\n",
    "        for i in links:\n",
    "            i=i.text\n",
    "            if 'https' in i:\n",
    "                link_contact.append(i.split(' › ')[0]+'/contact')\n",
    "                \n",
    "        for i in links:\n",
    "            i=i.text\n",
    "            if 'https' in i:\n",
    "                link.append(i.replace(' › ','/'))\n",
    "\n",
    "\n",
    "        names=driver.find_elements(By.XPATH,'//span[@class=\"VuuXrf\"]')\n",
    "        for i in names:\n",
    "            i=i.text \n",
    "            if i != '':\n",
    "                name.append(i)\n",
    "        print(f'page scrape {c}')\n",
    "        c+=1\n",
    "\n",
    "        driver.find_element(By.XPATH,'//span[@style=\"display:block;margin-left:53px\"]').click()\n",
    "        sleep(0.5)\n",
    "except:\n",
    "    print('Page end')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4e2b06-79a6-4a7b-b8d8-3af59245db90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd324a7f-4f7d-4c88-98f4-62863104054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page scrape 1\n",
      "page scrape 2\n",
      "page scrape 3\n",
      "page scrape 4\n",
      "Page end\n"
     ]
    }
   ],
   "source": [
    "# link_contact1=[]\n",
    "# link1=[]\n",
    "# name1=[]\n",
    "c=1\n",
    "try:\n",
    "    for n in range(30):\n",
    "        links=driver.find_elements(By.XPATH,'//cite[@class=\"apx8Vc qLRx3b tjvcx GvPZzd cHaqb\"]')\n",
    "        for i in links:\n",
    "            i=i.text\n",
    "            if 'https' in i:\n",
    "                link_contact.append(i.split(' › ')[0]+'/contact')\n",
    "                \n",
    "        for i in links:\n",
    "            i=i.text\n",
    "            if 'https' in i:\n",
    "                link.append(i.replace(' › ','/'))\n",
    "\n",
    "\n",
    "        names=driver.find_elements(By.XPATH,'//span[@class=\"VuuXrf\"]')\n",
    "        for i in names:\n",
    "            i=i.text \n",
    "            if i != '':\n",
    "                name.append(i)\n",
    "        print(f'page scrape {c}')\n",
    "        c+=1\n",
    "\n",
    "        driver.find_element(By.XPATH,'//span[@style=\"display:block;margin-left:53px\"]').click()\n",
    "        sleep(0.5)\n",
    "except:\n",
    "    print('Page end')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "476402a1-9602-44ec-b94a-18943cf513dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0295bcd1-062c-4f2b-8c5f-dfa04ff3e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[]\n",
    "for i in link:\n",
    "     k.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "952b478c-e6fa-4dd4-b112-38634f11069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stadium Rant\n",
      "justdancestudios.co.uk\n",
      "anywearprinting.com\n",
      "nancyshopecorner.com\n",
      "thepeoplesstore.org\n",
      "Landform Projects Ltd\n",
      "Rhino Bullets\n",
      "bhambori.com\n"
     ]
    }
   ],
   "source": [
    "for i in name:\n",
    "     if i != '':\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5f9a406-5b86-4bab-8c48-83d0043ac0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[]\n",
    "for i in link:\n",
    "    if 'https' in i:\n",
    "        k.append(i.split(' › ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe5a2d0d-b58a-4738-9af6-2ac41f9a3cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.stadiumrant.com',\n",
       " 'https://www.justdancestudios.co.uk',\n",
       " 'https://www.anywearprinting.com',\n",
       " 'https://www.nancyshopecorner.com',\n",
       " 'https://www.thepeoplesstore.org',\n",
       " 'https://www.landformprojects.com',\n",
       " 'https://www.rhinobullets.co.za',\n",
       " 'https://www.bhambori.com']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e7b8a94-3291-4039-92dc-3d26a5a98f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stadium Rant',\n",
       " 'justdancestudios.co.uk',\n",
       " 'anywearprinting.com',\n",
       " 'nancyshopecorner.com',\n",
       " 'thepeoplesstore.org',\n",
       " 'Landform Projects Ltd',\n",
       " 'Rhino Bullets',\n",
       " 'bhambori.com',\n",
       " 'ROI ByDesign',\n",
       " 'artidilkursu.com',\n",
       " 'Wild State Boutique',\n",
       " 'starbwoykrakz.com',\n",
       " 'Handsome Goat Soaps',\n",
       " 'lannsann.com',\n",
       " 'bestsave.in',\n",
       " 'artofflames.com',\n",
       " 'shoppeonysky.com',\n",
       " 'travistranspo.com',\n",
       " 'nevascka.com.br',\n",
       " 'nzspirit.com',\n",
       " 'poetryartbookstation.com',\n",
       " 'artemis-chase.com',\n",
       " 'pficlub.com',\n",
       " 'minksnhoney.com',\n",
       " 'thecornishsweetshop.com',\n",
       " 'kaikitfood.com',\n",
       " 'maceehall.com',\n",
       " 'Fabmost.com',\n",
       " 'wickedwickmt.com',\n",
       " 'northseavikings.nl',\n",
       " 'fideliumwoodworking.com.au',\n",
       " 'benedettofamilylaw.com',\n",
       " 'mjhproduction.com',\n",
       " 'thequirkstreet.com',\n",
       " 'jmemedicos.com',\n",
       " 'wildflowercottage.net',\n",
       " 'aproxyshades.com',\n",
       " 'magnum-go.com',\n",
       " 'gearspunmedia.com',\n",
       " 'aimcareer.org',\n",
       " 'mdigitaldesigns.com',\n",
       " 'rainbowresinqueens.com',\n",
       " 'highkeyblessed.com',\n",
       " 'thecanapeclubportugal.com',\n",
       " 'wcbundles.com',\n",
       " 'heytheresweetcheeks.com',\n",
       " 'escapewhileyoucan.com',\n",
       " 'insightsinbits.com',\n",
       " 'littlebigfund.com',\n",
       " 'plasterrockgolfclub.com',\n",
       " 'feledhetetlen.com',\n",
       " 'elevatedeyewear.net',\n",
       " 'dreamhairwellness.in',\n",
       " 'kingpopoff.com',\n",
       " 'allworldoutlet.com',\n",
       " 'relentlessstoic.com',\n",
       " 'tomhillvisuals.com',\n",
       " 'dasysglobal.com',\n",
       " 'cookmedaddy.com',\n",
       " 'paw-paw-unlimited.com',\n",
       " 'vestdoctor.com',\n",
       " 'angelshao-website.com',\n",
       " 'studiofitness32.com',\n",
       " 'hairvitality.net',\n",
       " 'Toy-wave Japan',\n",
       " 'howiedigital.com',\n",
       " 'hollowgrove.shop',\n",
       " 'htfoxdesigns.com',\n",
       " 'horizondrone.services',\n",
       " 'bluedoordecorllc.com',\n",
       " 'greenboxassociates.com',\n",
       " 'wtbtherapy.com',\n",
       " 'ecsaktifyasam.com',\n",
       " 'vielariat.com',\n",
       " 'grynn.co',\n",
       " 'communotperinatal.com',\n",
       " 'judonj.org',\n",
       " 'communotperinatal.com',\n",
       " 'judonj.org',\n",
       " '72shirts.net',\n",
       " 'songsbysarah.com',\n",
       " 'thelittlelifterco.com.au',\n",
       " 'sandwichstoptruck.com',\n",
       " 'wrightssolarcleaning.com',\n",
       " 'bearsonics.org',\n",
       " 'woodslats.com.au',\n",
       " 'opcbyky.com',\n",
       " 'crunitedcontracting.com',\n",
       " 'riseconsultancygroup.com',\n",
       " 'coreloveco.com',\n",
       " 'ravensmom.com',\n",
       " 'ttfwodonga.com.au',\n",
       " 'decorshowcase.in',\n",
       " 'vivalacali.com.au',\n",
       " 'Devoted Disciple',\n",
       " 'plumbtechincok.com',\n",
       " 'tormentavolleyball.com',\n",
       " 'gashmerch.co.uk',\n",
       " 'stitchingittogethersustainably.com',\n",
       " 'innerstellarfacepaint.com',\n",
       " 'hardtdetailing.com',\n",
       " 'carrotsmillworks.com',\n",
       " 'fidelitystrategicconsulting.com',\n",
       " 'creative-faces.co.uk',\n",
       " 'Circus Wonderland',\n",
       " 'holychaosdesign.com',\n",
       " 'blueherontraining.com',\n",
       " 'casonquilting.com',\n",
       " 'foxwisellc.com',\n",
       " 'klpcleaningservices.co.uk',\n",
       " 'stricklycleanllc.com',\n",
       " 'waialuawrestlingclub.com',\n",
       " 'jobinterviewpreparationlondon.com',\n",
       " 'hodgehaulinganddispatch.com',\n",
       " 'daspunkhaus.com',\n",
       " 'healingwithnamah.com',\n",
       " 'grumpy-gatorz.com',\n",
       " 'compassiontelehealth-np.com',\n",
       " 'stillwaterpianostudio.com',\n",
       " 'Wix.com',\n",
       " 'horseedenterprise.com',\n",
       " 'herjourneymatters.ca',\n",
       " 'bindingandblessedceremonies.com.au',\n",
       " 'abcparenting.co',\n",
       " 'pivovar-hm.com',\n",
       " 'jamienollrealtor.com',\n",
       " 'ktfbeauty.co.uk',\n",
       " 'rhythmsoul.shop',\n",
       " 'newenglandstormcenter.com',\n",
       " 'emmanuellebailey.co.uk',\n",
       " 'willowswildwest.com',\n",
       " 'atlasprojects.net',\n",
       " 'jeanninecaineart.com',\n",
       " 'ralphandroselifestyle.com',\n",
       " 'gurophotography.com',\n",
       " 'e-coolmerce.com',\n",
       " 'dripee.lt',\n",
       " 'dripee.lt',\n",
       " 'benjaminbelloir.com',\n",
       " 'iansplopshop.com',\n",
       " 'edara-e-sharia.com',\n",
       " 'goalpostgambling.com',\n",
       " 'alegend.co',\n",
       " 'stickerclu8.com',\n",
       " 'tashyoung.com',\n",
       " 'ingeniously.co',\n",
       " 'urbancgy.com',\n",
       " 'masonautocare.com',\n",
       " 'immigrantyouthsoccertpp.org',\n",
       " 'appliedpressure2023.com',\n",
       " 'turncobuilding.com.au',\n",
       " 'randscarpentryservice.com',\n",
       " 'designdojo.se']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543748d-66fb-43b2-9d60-f63ed0aa8d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
